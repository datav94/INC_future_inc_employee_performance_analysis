{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2953e9e-95ae-4b6d-be7a-ffe7da2912a6",
   "metadata": {},
   "source": [
    "# __Employee Performance Analysis__\n",
    "## __INX Future Inc.__\n",
    "### __Train Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dcda289-c2d5-49c6-86c9-db4afd8c2f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general purpose libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# data loading and wrangling libraries for EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data balancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ml models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# ML model evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load the dot env file that contains the path to data file for data privacy\n",
    "dotenv_path = os.getcwd()+'\\\\local.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6e93e-f41e-41a7-8d6f-33f64e47b045",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a11a479e-26c0-44fb-aa61-8b52c6755b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmpNumber</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>EducationBackground</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>EmpDepartment</th>\n",
       "      <th>EmpJobRole</th>\n",
       "      <th>BusinessTravelFrequency</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>EmpEducationLevel</th>\n",
       "      <th>EmpEnvironmentSatisfaction</th>\n",
       "      <th>EmpHourlyRate</th>\n",
       "      <th>EmpJobInvolvement</th>\n",
       "      <th>EmpJobLevel</th>\n",
       "      <th>EmpJobSatisfaction</th>\n",
       "      <th>NumCompaniesWorked</th>\n",
       "      <th>OverTime</th>\n",
       "      <th>EmpLastSalaryHikePercent</th>\n",
       "      <th>EmpRelationshipSatisfaction</th>\n",
       "      <th>TotalWorkExperienceInYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>EmpWorkLifeBalance</th>\n",
       "      <th>ExperienceYearsAtThisCompany</th>\n",
       "      <th>ExperienceYearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>PerformanceRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E1001000</td>\n",
       "      <td>32</td>\n",
       "      <td>Male</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Single</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E1001006</td>\n",
       "      <td>47</td>\n",
       "      <td>Male</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Single</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E1001007</td>\n",
       "      <td>40</td>\n",
       "      <td>Male</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>Married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EmpNumber  Age Gender  ... YearsWithCurrManager Attrition PerformanceRating\n",
       "0  E1001000   32   Male  ...                    8        No                 3\n",
       "1  E1001006   47   Male  ...                    7        No                 3\n",
       "2  E1001007   40   Male  ...                   12        No                 4\n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.read_excel(os.getenv('data'))\n",
    "main_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a7131-5160-496e-8b9e-e0a2785f58c3",
   "metadata": {},
   "source": [
    "## Preprocessing the data for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adaad323-fb03-4617-9ab4-0a4140689fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets prepare our dataset\n",
    "\n",
    "ml_df = main_df.drop('EmpNumber', axis=1)\n",
    "ml_df.head(2)\n",
    "\n",
    "cat_df = ml_df.select_dtypes(['object','bool'])\n",
    "num_df = ml_df.select_dtypes(['int64', 'float64'])\n",
    "\n",
    "# OneHotEncoding of Categorical features\n",
    "cat_df = pd.get_dummies(cat_df)\n",
    "\n",
    "# concatonate the categorical and numberical datasets to make one prepared dataset for RandomForest model\n",
    "ml_df = pd.concat([cat_df, num_df], axis=1)\n",
    "\n",
    "# Splitting the dataset into X=features and y=target \n",
    "X = ml_df.iloc[:, :-1]\n",
    "y = ml_df.PerformanceRating\n",
    "\n",
    "# now lets perform the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.75, random_state=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751720a-5385-4a13-b159-4129053039fe",
   "metadata": {},
   "source": [
    "## Balancing the dataset using imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35c8c74-92a8-474f-9706-8c3d2385e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling = SMOTE()\n",
    "undersampling = RandomUnderSampler()\n",
    "\n",
    "steps = (('o', oversampling), ('u', undersampling))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "X_bal, y_bal = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3b728-ecf0-43a5-9009-93e43640e365",
   "metadata": {},
   "source": [
    "## Model selection using K-Fold Cross validation\n",
    "\n",
    "K-Fold cross validation processing the data models in such a way that the model does not fall into the traps of overfitting as it creates K validation splits and performs model training and accuracy evaluation on K splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3383d09f-a934-4560-be2e-262fd770670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a model dictionary that will be used by cross validation to perform K-Fold on each of the models\n",
    "models = {\n",
    "    'logr': LogisticRegression(),\n",
    "    'knn': KNeighborsClassifier(n_neighbors=2),\n",
    "    'Kmeans': KMeans(n_clusters=3),\n",
    "    'naive_bayes': GaussianNB(),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(128, 96, 32, 3), activation='relu'),\n",
    "    'RandomF': RandomForestClassifier(n_estimators=800),\n",
    "    'GrdBoostClassifier': GradientBoostingClassifier(n_estimators=800),\n",
    "    'XGBoost': XGBClassifier(n_estimators=900, objective='multi:softprob', eval_metric='merror')\n",
    "}\n",
    "\n",
    "model_report = {}\n",
    "for key, value in models.items():\n",
    "    \n",
    "    # lets first create k folds of our dataset\n",
    "    kfold = KFold(n_splits=10, random_state=15, shuffle=True)\n",
    "    \n",
    "    # now lets train and varify each model's cross validations score\n",
    "    cv_report = cross_val_score(value, X_train, y=y_train, cv=kfold, scoring='accuracy')\n",
    "    model_report.update(**{\n",
    "        key: (cv_report, cv_report.mean(), cv_report.std())\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cdf85-7397-4cf0-90c9-0bdf6faccfcf",
   "metadata": {},
   "source": [
    "## Lets plot the results to find out the best performing model so that we can further perform Hyperparameter tuning for that model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be089257-fbe6-432c-8121-a380037a46d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,12))\n",
    "\n",
    "fig.suptitle('ML Model Performance')\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.boxplot([value[0] for value in model_report.values()])\n",
    "ax.set_xticklabels([key for key in model_report.keys()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ed1de-f7d3-43ac-a992-18f1d175522f",
   "metadata": {},
   "source": [
    "## Selected ML models\n",
    "\n",
    "The winner here are clearly __GradientBoostClassifier__ and __XGBClassifier__ who have won by a great margin and hence our selected models for training are:\n",
    "\n",
    "1. __GradientBoostClassifier__\n",
    "2. __XGBClassifier__\n",
    "\n",
    "- Now lets perform Hyperparameter tuning for these two models to figure out the best model and best parameters for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb0965-86d9-46ab-bea5-64c8172b49eb",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning on GradientBoostClassifier\n",
    "\n",
    "we shall use the sklearn's GridSearchCV to perform hyperparameter tuning and we shall use the parameter grid to select the best combination of parameters to provide us with the best possible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "147bc400-0c47-4707-959a-b98a699c9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c278cc6c-7cd4-4c8a-9c77-4155afddb226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, estimator=GradientBoostingClassifier(), n_jobs=1,\n",
       "                   param_distributions={'criterion': ['friedman_mse',\n",
       "                                                      'squared_error', 'mse',\n",
       "                                                      'mae'],\n",
       "                                        'learning_rate': [0.1, 0.2],\n",
       "                                        'max_depth': [9, 10, 11],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [2, 3],\n",
       "                                        'n_estimators': [700, 800, 900],\n",
       "                                        'subsample': [0, 1]},\n",
       "                   return_train_score=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets first create the hyperparameter grid\n",
    "\n",
    "gb_params ={\n",
    "    'n_estimators': [700, 800, 900],\n",
    "    'learning_rate' : [0.1,0.2],\n",
    "    'max_depth': [9, 10, 11],\n",
    "    'min_samples_leaf': [2,3],\n",
    "    'subsample': [0,1],\n",
    "    'max_features' : ['auto','sqrt','log2'],\n",
    "    'criterion': ['friedman_mse', 'squared_error', 'mse', 'mae']\n",
    "}\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model, \n",
    "    gb_params, \n",
    "    cv=4, \n",
    "    n_jobs=1, \n",
    "    verbose=0, \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "random_search.fit(X_bal, y_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f84bc65-b14d-42ba-895a-2025e4963033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 1, 'n_estimators': 700, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 11, 'learning_rate': 0.1, 'criterion': 'mae'}\n",
      "0.9633772257967979\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0dd00b-b254-4ddc-ae8b-00c3225f6377",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning on XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d83dfb3-9d2e-4510-bb13-7e6889c53daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:47:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:47:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:48:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"max_features\", \"min_samples_leaf\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:48:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'subsample': 1, 'n_estimators': 900, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 11, 'learning_rate': 0.1, 'criterion': 'friedman_mse'}\n",
      "0.9603003890468352\n"
     ]
    }
   ],
   "source": [
    "# lets first create the hyperparameter grid\n",
    "\n",
    "xgb_params ={\n",
    "    'n_estimators': [400 ,500, 600, 700, 800, 900],\n",
    "    'booster': ['gbtree','dart'],\n",
    "    'use_label_encoder': True, \n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.3],\n",
    "    'max_depth': [7, 8, 9, 10, 11, 12, 14],\n",
    "    'tree_method': ['auto','exact','approx','hist','gpu_hist'],\n",
    "    'objective' : 'multi:softprob',\n",
    "    'n_jobs': -1,\n",
    "    'eval_metric': 'merror'\n",
    "}\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model, \n",
    "    gb_params, \n",
    "    cv=4, \n",
    "    n_jobs=1, \n",
    "    verbose=0, \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "random_search.fit(X_bal, y_bal)\n",
    "\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef519727-fddd-4602-97bb-ac5a3361c113",
   "metadata": {},
   "source": [
    "## Conclusion we shall use the parameters provided by RandomizedSearchCV for training our models\n",
    "\n",
    "## Further hyperparameter tuning\n",
    "- A futher tuning of these hyperparameters can be gained using GridSearchCV as that will give out thee best available parametric values for our models but it has not been used here due to the compute time and resource restrictions as it takes up more time and more resources to perform hyperparameter tuning as it tries out all possible combinations of parameters provided in the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3e0c5-c909-46ad-86c2-996f6833c80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
